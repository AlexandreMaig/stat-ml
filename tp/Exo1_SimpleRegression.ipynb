{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 1 : Utilisation de scikit-learn pour la regression lineaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation de donnees test\n",
    "n = 100\n",
    "x = np.arange(n)\n",
    "y = np.random.randn(n)*30 + 50. * np.log(1 + np.arange(n))\n",
    "\n",
    "# instanciation de sklearn.linear_model.LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x[:, np.newaxis], y)  # np.newaxis est utilise car x doit etre une matrice 2d avec 'LinearRegression'\n",
    "\n",
    "# representation du resultat\n",
    "fig = plt.figure()\n",
    "plt.plot(x, y, 'r.')\n",
    "plt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\n",
    "plt.legend(('Data', 'Linear Fit'), loc='lower right')\n",
    "plt.title('Linear regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">QUESTION 1.1 :</span> \n",
    "Expliquer ce qu'est *lr* et ce que font *lr.fit* et *lr.predict*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 1.1 :</span> \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">QUESTION 1.2 :</span> \n",
    "\n",
    "On s'interesse à x=105. En supposant que le model lineaire soit toujours valide pour ce x, quelle valeur corresondante de y vous semble la plus vraisemblable ? \n",
    "\n",
    "On remarquera que les valeurs données pour la prediction doivent être dans un vecteur colonne, ici une matrice 1x1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 1.2 :</span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PARTIE 2 : maximum de vraisemblance\n",
    "\n",
    "### <span style=\"color:blue\">QUESTION 2.1 :</span> \n",
    "\n",
    "Tirer 10 fois une pièce à pile ou face et modéliser les résultats obtenus comme ceux d'une variable aléatoire X qui vaut X_i=0 si on a pile et X_i=1 si on a face.\n",
    "\n",
    "Calculez le maximum de vraisemblance du paramètre p d'un loi de Bernoulli qui modéliserait le problème. Pour y arriver, différentes valeures possibles de p seront testées et le p retenu sera celui qui a la plus grande vraisemblance.\n",
    "\n",
    "\n",
    "- Vérifier empiriquement comment évolue ce maximum de vraisemblance si l'on effectue de plus en plus de tirages\n",
    "- Que se passe-t-il quand il y a trop de tirages ? Représenter la log-vraisemblance plutot que la vraisemblance dans ce cas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NbTirages=100\n",
    "NbPiles = np.random.randn(NbTirages) > 0\n",
    "NbPiles = np.sum(NbPiles)\n",
    "NbFaces=NbTirages-NbPiles\n",
    "\n",
    "\n",
    "PossibleValuesForP=np.linspace(0.01,0.99,100)\n",
    "\n",
    "\"\"\"\n",
    "def vraisemblance(n_pile, n_face, p):\n",
    "    \n",
    "    return ... \n",
    "\"\"\" \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 2.2 :</span> \n",
    "\n",
    "\n",
    "Vérifier empiriquement comment évolue ce maximum de vraisemblance si l'on effectue de plus en plus de tirages. Pour éviter de tirer des centaines de fois à pile ou face, vous pourrez juste modifier *NbTirages¨et *NbPiles* dans le code, puis voir le comportement de la courbe *plt.plot(PossibleValuesForP,CorrespondingLikelihood)*.\n",
    "\n",
    "\n",
    "### <span style=\"color:blue\">REPONSE 2.2 :</span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "...\n",
    "\n",
    "### <span style=\"color:blue\">QUESTION 2.3 :</span> \n",
    "\n",
    "\n",
    "Que se passe-t-il quand il y a trop de tirages ? Représenter la log-vraisemblance plutot que la vraisemblance dans ce cas.\n",
    "\n",
    "### <span style=\"color:blue\">REPONSE 2.3 :</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NbTirages=2000\n",
    "NbPiles = np.random.randn(NbTirages) > 0\n",
    "NbPiles = np.sum(NbPiles)\n",
    "NbFaces=NbTirages-NbPiles\n",
    "\n",
    "\n",
    "PossibleValuesForP=np.linspace(0.01,0.99,100)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PARTIE 3 : maximum de vraisemblance 2\n",
    "\n",
    "On veut quantifier le lien entre deux variables pour lesquelles une relation lineaire semble exister, mais le bruit qui effecte les observations sur la deuxieme variable n'est clairement pas Gaussien.\n",
    "\n",
    "Nous allons essayer de trouver la relation entre les variables a l'aide d'un modele de regression lineaire dont les parametres seront optimises en maximisant une vraissemblance plutot qu'en minimisant une erreur au carre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MyData=np.genfromtxt('QuantifiedDataExo1_3.csv')\n",
    "\n",
    "plt.scatter(MyData[:,0],MyData[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 3.1 :</span> \n",
    "\n",
    "Essayez de resoudre le probleme a l'aide de l'algorithme de regression lineaire de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=MyData[:,0]\n",
    "Y=MyData[:,1]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater que la pente de la courbe est legerement trop forte. Cette mauvaise estimation est due a trois observations a droite de la figure qui font un effet levier. La regression lineaire minimise l'erreur d'approximation au carre sur les observations d'apprentissage. De maniere sous-jacente cela se base sur l'hypothese que les erreurs d'approximation suivent une loi normale centree (et pas forcement reduite). Hors, les erreurs d'approximation autour d'un modele lineaire sont clairement non symetriques ici. Nous allons alors resoudre le probleme au sens du maximum de vraisemblance.\n",
    "\n",
    "### <span style=\"color:blue\">QUESTION 3.2 :</span> \n",
    "\n",
    "On va modeliser le probleme sous la forme :\n",
    "\n",
    "$ypred_i = a * x_i +b \\,,\\, \\forall i = 1, \\ldots, n$\n",
    "et $err_i= ypred_i-y_i$\n",
    "\n",
    "ou les $x_i$ et $y_i$ sont les donnees d'apprentissage pour les observations $i$ dans $[1, 2, ..., n]$, et $ypred_i$ approche $y_i$. Les deux parametres du modele lineaire que l'on cherche a estimer sont $a$ et $b$. Afin de résoudre le probleme, on va alors repondre aux sous-questions suivantes :\n",
    "\n",
    "#### <span style=\"color:blue\">QUESTION 3.2.1 :</span> \n",
    "Codez une fonction qui calcul les erreurs d'approximations pour toutes les observations de $X$ et $Y$ avec un $a$ et un $b$ specifiques.\n",
    "\n",
    "#### <span style=\"color:blue\">QUESTION 3.2.2 :</span> \n",
    "\n",
    "Codez une fonction qui calcule la vraisemblance de parametres pour lesquel l'erreur d'approximation suit une loi normale centree d'ecart type sigma. On donnera la valeur par defaut sigma=2\n",
    "\n",
    "#### <span style=\"color:blue\">QUESTION 3.2.3 :</span> \n",
    "\n",
    "Codez une fonction qui calcule la vraisemblance de parametres pour lesquel l'erreur d'approximation suit une loi de chi2. On fixera par defaut le nombre de degres de liberte ddl=3 et l'echelle de la loi (scale) a 0.4. On fera très attention au fait que la densite de probabilite d'une valeur negative sera egale a zero avec la loi du chi2.\n",
    "\n",
    "#### <span style=\"color:blue\">QUESTION 3.2.4 :</span> \n",
    "\n",
    "Utilisez les fonctions de calcul de la vraisemblance pour trouver une relation lineaire qui semble raisonable, i.e. pour trouver les parametres a et b les plus vraisemblables. On pourra eventuellement s'aider d'une representation du nuage de points qui represente le 'score' attribue a chaque observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE 2.1\n",
    "def compute_errors(X,Y,theta):\n",
    "    \"\"\"\n",
    "    returns a vector having the same size as X or Y which represents the errors\n",
    "    with a 1D linear model of parameters theta=[a,b]\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CODE 2.2\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def likelihood_normal(X,Y,theta,sigma=2.):\n",
    "    \"\"\"\n",
    "    returns the likelihood of the 1D linear model with parameters theta=[a,b] and\n",
    "    the errors following a normal law of std=sigma\n",
    "    \"\"\"\n",
    "\n",
    "    errors=compute_errors(X,Y,theta)\n",
    "\n",
    "    ...\n",
    "\n",
    "    return likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE 2.3\n",
    "\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def likelihood_chi2(X,Y,theta,dof=3,sc=0.4):\n",
    "    \"\"\"\n",
    "    returns the likelihood of the 1D linear model with parameters theta=[a,b] and\n",
    "    the errors following a chi2 law of dof degrees of freedom\n",
    "    \"\"\"\n",
    "\n",
    "    errors=compute_errors(X,Y,theta)\n",
    "\n",
    "    ...\n",
    "\n",
    "    return likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE 2.4\n",
    "\n",
    "likelihood_normal(X,Y,[1.2,-0.3],sigma=2.,verbose=True)\n",
    "\n",
    "likelihood_chi2(X,Y,[1.2,-0.6],dof=3,sc=0.4,verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 3.3 :</span> \n",
    "\n",
    "Codez une fonction de descente de gradient pour apprendre les parametres optimaux du modele (a et b) avec les deux types de bruit consideres mais leurs parametres fixes aux valeurs par defaut.\n",
    "\n",
    "Remarque: on pourra +maximiser+ la +log-vraisemblance+, ce qui est numeriquement plus simple que la vraisemblance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MinusLogLikelihood_normal(X,Y,theta,sigma=2.):\n",
    "    return -np.log(likelihood_normal(X,Y,theta,sigma=sigma)) #ne resoud pas les pbs numeriques si n est grand, mais plus sympatique pour la descente de gradient\n",
    "\n",
    "def MinusLogLikelihood_chi2(X,Y,theta,dof=3,sc=0.4):\n",
    "    return -np.log(likelihood_chi2(X,Y,theta,dof=dof,sc=sc)) #ne resoud pas les pbs numeriques si n est grand, mais plus sympatique pour la descente de gradient\n",
    "\n",
    "#3.1: fonctions pour la descente de gradient\n",
    "\n",
    "def Grad_function(f,X,Y,theta_loc,epsilon=1e-5):\n",
    "  ...\n",
    "  return ApproxGrad\n",
    "\n",
    "#descente de gradient avec alpha defini a la main\n",
    "\n",
    "def grad_descent(funct,X,Y,theta_init,convspeedfactor=0.1,nbIterations=100):\n",
    "    ...\n",
    "\n",
    "    return theta\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "l'optimisation marche aussi, meme si il faut faire tres attention au choix du theta_init (la densite du chi2 est nulle pour les erreurs negatives)... par contre :\n",
    "  -> la vraisemblance est de 10e-7 au lieu de 10e-15\n",
    "  -> le modele lineaire colle mieux a la majorite des donnees et est moins sensible aux donnees visiblement aberrantes\n",
    "\n",
    "\n",
    "On peut alors plus faire confiance a la pente calculee avec le bruit de type chi2 que le bruit gaussien, MAIS en etant clair sur le fait qu'on aura une tendance loin d'etre negligeable de s'eloigner regulierement du modele lineaire de maniere non-symetrique par rapport au modele.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Remarque : Generation de donnees dans cet exercice :\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 20\n",
    "X = np.random.uniform(size=n)\n",
    "y = 1.2*X[:] + np.random.chisquare(1.,size=n)*0.4-0.5\n",
    "\n",
    "plt.plot(X[], y, 'r.')\n",
    "plt.show()\n",
    "\n",
    "MyData=np.concatenate((X.reshape(-1,1),y.reshape(-1,1)),axis=1)\n",
    "np.savetxt('J18_E4_QuantifiedData.csv',MyData)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PARTIE 4 : impact et detection d'outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation de donnees test\n",
    "def generate_data(n_samples, outlier=False, b_1=4.):\n",
    "    x = np.arange(n_samples)\n",
    "    y = 10. + b_1*x + np.random.randn(n_samples)*3.\n",
    "    if outlier:\n",
    "        y[-1] += 20\n",
    "    return x, y\n",
    "\n",
    "def s2(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    SSE = np.sum((y_true - y_pred)**2)\n",
    "    return SSE / (n-1)\n",
    "\n",
    "\n",
    "x, y = generate_data(n_samples=10, outlier=True)\n",
    "# instanciation de sklearn.linear_model.LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x[:, np.newaxis], y)  # np.newaxis est utilise car x doit etre une matrice 2d avec 'LinearRegression'\n",
    "\n",
    "# representation du resultat\n",
    "\n",
    "print('b_0='+str(lr.intercept_)+' et b_1='+str(lr.coef_[0]))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x, y, 'r.')\n",
    "plt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\n",
    "plt.legend(('Data', 'Linear Fit'), loc='lower right')\n",
    "plt.title('Linear regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 4.1 :</span> \n",
    "\n",
    "\n",
    "Remarquons que la ligne 'y[9]=y[9]+20' génere artificiellement une donnée aberrante.\n",
    "\n",
    "Tester l'impact de la donnée aberrante en estimant b_0, b_1 et s^2 sur \n",
    "- 5 jeux de données générés comme dans la cellule précédente et\n",
    "- 5 autres jeux aussi générés suivant cette méthode, mais sans la données aberrant (simplement ne pas executer la ligne y[9]=y[9]+20).\n",
    "\n",
    "On remarque que $\\beta_0 = 10$, $\\beta_1 = 4$ et $\\sigma=3$ dans les données simulees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 4.1 :</span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variance estimée du bruit est beaucoup plus grande avec une donnée aberrante.\n",
    "La donnée aberrante introiduit un biais dans l'estimation des statistiques b_0 et b_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTIONS 4.2 :</span> \n",
    "\n",
    "#### <span style=\"color:blue\">QUESTION 4.2.a :</span> \n",
    "Pour chaque variable i, calculez les profils des résidus $e_{(i)j}=y_j - \\hat{y_{(i)j}}$ pour tous les j, où  \\hat{y_{(i)j}} est l'estimation de y_j à partir d'un modele  linéaire appris sans l'observation i.\n",
    "#### <span style=\"color:blue\">QUESTION 4.2.b :</span> \n",
    "En quoi le profil des e_{(i)j} est différent pour i=9 que pour les autres i\n",
    "#### <span style=\"color:blue\">QUESTION 4.2.c :</span> \n",
    "Etendre ces calculs pour définir la distance de Cook de chaque variable i\n",
    "\n",
    "AIDE : pour enlever un élement 'i' de 'x' ou 'y', utiliser x_del_i=np.delete(x,i) et y_del_i=np.delete(y,i) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data(n_samples=10, outlier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">REPONSE 4.2.a :</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profil_residuel(x, y, i):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">REPONSE 4.2.b :</span> \n",
    "\n",
    "Le profil résiduel de la donnée aberrante est toujours plus grand que les autres, en particulier lorsqu'on ôte la donnée de la base d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook(x, y, i):\n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">REPONSE 4.2.c :</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un jeu de données de 20 observations obtenues avec un coefficient directeur $\\beta_1$ de $0.2$, faire un test d'hypothèse pour vérifier que les données sont corrélées avec une confiance de 95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle que sous les hypothèses suivantes : \n",
    " * $\\mathbb{E}[\\epsilon_i] = 0$,\n",
    " * $\\mathbb{V}[\\epsilon_i] = \\sigma^2$,\n",
    " * $\\forall i \\neq j, \\: Cov(\\epsilon_i, \\epsilon_j) = 0$,\n",
    " \n",
    "on a : \n",
    "\n",
    "* $\\mathbb{E}[\\hat{\\beta_0}] = \\beta_0$,\n",
    "* $\\mathbb{E}[\\hat{\\beta_1}] = \\beta_1$,\n",
    "* $\\mathbb{V}[\\hat{\\beta_0}] = \\sigma^2(\\frac{1}{n} + \\frac{\\bar{x_n}^2}{\\sum_{i=1}^n (x_i - \\bar{x_n})^2})$,\n",
    "* $\\mathbb{V}[\\hat{\\beta_1}] = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x_n})^2}$\n",
    "\n",
    "En faisant l'hypothèse supplémentaire que les erreurs suivent une loi normale, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, on a :\n",
    "\n",
    "* $\\hat{\\beta_1} \\sim \\mathcal{N}(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x_n})^2})$,\n",
    "* $\\frac{(n-2) \\cdot s^2}{\\sigma^2} \\sim \\mathcal{X}^2(n-2)$,\n",
    "* $\\hat{\\beta_1}$ et $s^2$ indépendants,\n",
    "\n",
    "où $s^2 = \\frac{\\sum_{i=1}^n \\hat{\\epsilon_i}^2}{n-2}$ est un estimateur non biaisé de $\\sigma^2$.\n",
    "\n",
    "On peut en déduire que:\n",
    "\n",
    "$$ \\frac{ \\frac{ \\hat{\\beta_1}-\\beta_1}{\\sqrt{\\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x_n})^2}}}}{\\sqrt{\\frac{\\frac{(n-2)s^2}{\\sigma^2}}{n-2}}} = \\frac{\\hat{\\beta_1}-\\beta_1}{\\frac{s}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x_n})^2}}} = T_n \\sim T(n-2)$$ \n",
    "\n",
    "où $T(n-2)$ désigne la loi de Student à $n-2$ degrés de liberté, d'espérance nulle si $n-2 > 1$.\n",
    "\n",
    "Tester l'hypothèse H_0 : $\\beta_1 = 0$ en prenant un risque de 5%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 5 : Vers la regression linéaire multiple et optimisation\n",
    "\n",
    "On considère que l'on connait les notes sur une année de n élèves dans p matières, ainsi que leurs notes à un concours en fin d'annee. L'année suivante, on  se demande si on ne pourrait pas prédire la note des étudiants au concours en fonction de leurs notes annuelle afin d'estimer leurs chances de réussite au concours.\n",
    "\n",
    "\n",
    "On va resoudre le problème à l'aide de la régression linéaire en dimension p>1 sans utiliser scikit-learn. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">QUESTION 5.1 :</span> \n",
    "\n",
    "A l'aide de la fonction 'SimulateObservations', simulez un jeu de donnees d'apprentissage [X_l,y_l] avec 30 observations et un jeu de test [X_t,y_t] avec 10 observations. Les observations seront en dimension p=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateObservations(n_train,n_test,p):\n",
    "    \"\"\"\n",
    "    n_train: number of training obserations to simulate\n",
    "    n_test: number of test obserations to simulate\n",
    "    p: dimension of the observations to simulate\n",
    "    \"\"\"\n",
    "\n",
    "    ObsX_train=20.*np.random.rand(n_train,p)\n",
    "    ObsX_tst=20.*np.random.rand(n_test,p)\n",
    "\n",
    "    RefTheta=np.random.rand(p)**3\n",
    "    RefTheta=RefTheta/RefTheta.sum()\n",
    "    print(\"The thetas with which the values were simulated is: \"+str(RefTheta))\n",
    "\n",
    "    ObsY_train=np.dot(ObsX_train,RefTheta.reshape(p,1))+1.5*np.random.randn(n_train,1)\n",
    "    ObsY_tst=np.dot(ObsX_tst,RefTheta.reshape(p,1))+1.5*np.random.randn(n_test,1)\n",
    "\n",
    "    return [ObsX_train,ObsY_train,ObsX_tst,ObsY_tst,RefTheta]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 5.1 :</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 5.2 :</span> \n",
    "\n",
    "On considere un modele linéaire en dimension p>1 pour mettre en lien les x[i,:] et les y[i], c'est a dire que np.dot(x[i,:],theta_optimal) doit etre le plus proche possible de y[i] sur l'ensemble des observations i. Dans le modèle linéaire multiple, theta_optimal est un vecteur de taille [p,1] qui pondère les différentes variables observées (ici les moyennes dans une matière). Coder alors une fonction qui calcule la moyenne des différences au carré entre ces valeurs en fonction de theta, *i.e.* la mean squared error (MSE) du modèle.\n",
    "\n",
    "### <span style=\"color:blue\">REPONSE 5.2 :</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CptMSE(theta_test, X, y_true):\n",
    "    y_pred = np.dot(X, theta_test)[:, np.newaxis]\n",
    "    MSE = np.mean((y_true - y_pred)**2)\n",
    "    return MSE\n",
    "\n",
    "theta_test=np.random.rand(p)\n",
    "theta_test=theta_test/theta_test.sum()\n",
    "\n",
    "MSE_test=CptMSE(theta_test, ObsX_train, ObsY_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 5.3 :</span> \n",
    "\n",
    "On va maintenant chercher le theta_test qui minimise cette fonction (il correspondra à theta_optimal), et ainsi résoudre le probleme d'apprentissage de regression lineaire multiple. Utiliser pour cela la fonction minimize de scipy.optimize\n",
    "\n",
    "\n",
    "De manière importante, la recherche des paramètres *theta_optimal* sera effectuée en utilisant les observations d'apprentissage (*ObsX_train* et *ObsY_train* en sortie *SimulateObservations*). La MSE obtenue sur les observations d'apprentissage avec *theta_optimal* sera comparée à celle obtenue avec les observations de test (*ObsX_tst* et *ObsY_tst* en sortie *SimulateObservations*) avec le même *theta_optimal*. Que constatez vous ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 5.3 :</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi optimiser le modèle par une descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CptMSE(X,y_true,theta_test):\n",
    "    y_pred=np.dot(X,theta_test)[:, np.newaxis]\n",
    "    #print(y_pred.shape)\n",
    "    #print(y_true.shape)\n",
    "    MSE=np.mean(np.power(y_pred-y_true,2.))\n",
    "\n",
    "    return MSE\n",
    "\n",
    "\n",
    "def gradientApprox(fct_to_minimize,theta_loc,X_loc,Y_loc,epsilon=1e-5):\n",
    "\n",
    "    fx=fct_to_minimize(X_loc,Y_loc,theta_loc)\n",
    "    #print(fx)\n",
    "    ApproxGrad=np.zeros(np.size(theta_loc))\n",
    "    veps=np.zeros(np.size(theta_loc))\n",
    "\n",
    "    for i in range(np.size(theta_loc)):\n",
    "        veps[:]=0.\n",
    "        veps[i]+=epsilon\n",
    "        ApproxGrad[i]=(fct_to_minimize(X_loc,Y_loc,theta_loc+veps)-fx)/epsilon\n",
    "    return ApproxGrad\n",
    "\n",
    "\n",
    "def analyticGradient(_, theta,x,y,epsilon=1e-5):\n",
    "    shape = theta.shape\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    grad_theta = (-2*np.dot((y-np.dot(x, theta)).T,x))/len(y)\n",
    "    grad_theta = grad_theta.reshape(shape)\n",
    "    return grad_theta\n",
    "\n",
    "def GradientDescent(fct_to_minimize,theta_init,X_loc,Y_loc,alpha=0.01,N=100, approx=True):\n",
    "    \"\"\"\n",
    "    Remark: the multiplicatory coefficient of the gradients will be \"alpha\" divided by the norm of the first gradient \n",
    "    \"\"\"\n",
    "\n",
    "    #init\n",
    "    l_thetas=[theta_init]\n",
    "    theta_curr=theta_init.copy()\n",
    "\n",
    "    #run the gradient descent\n",
    "    n=0\n",
    "    while n<N:\n",
    "        #approximate the gradient of fct_to_minimize w.r.t. theta_curr\n",
    "        if approx:\n",
    "            g=gradientApprox(fct_to_minimize,theta_curr,X_loc,Y_loc)\n",
    "        else:\n",
    "            g=analyticGradient(fct_to_minimize,theta_curr,X_loc,Y_loc)\n",
    "\n",
    "        #set the multiplicatory coefficient of the gradients\n",
    "        if n==0:\n",
    "            NormFirstGrads=np.linalg.norm(g)\n",
    "            coefMult=alpha/NormFirstGrads\n",
    "\n",
    "        #update theta\n",
    "        theta_curr=theta_curr-coefMult*g\n",
    "\n",
    "        #save the current state and increment n\n",
    "        l_thetas.append(theta_curr)\n",
    "        n+=1\n",
    "\n",
    "    return l_thetas\n",
    "\n",
    "theta_init = np.abs(np.random.randn(p))/10.\n",
    "thetas = GradientDescent(CptMSE, theta_init, ObsX_train, ObsY_train, approx=True)\n",
    "theta_optim = thetas[-1]\n",
    "\n",
    "mse_train = CptMSE(ObsX_train, ObsY_train, theta_optim)\n",
    "mse_test = CptMSE(ObsX_test, ObsY_test, theta_optim)\n",
    "\n",
    "mse_train_ref = CptMSE(ObsX_train, ObsY_train, RefTheta)\n",
    "mse_test_ref = CptMSE(ObsX_test, ObsY_test, RefTheta)\n",
    "\n",
    "print(\"Theta optimal --- \")\n",
    "print(\"Base d'apprentissage - MSE : \", mse_train)\n",
    "print(\"Base de test - MSE : \", mse_test)\n",
    "print(\"\")\n",
    "print(\"Theta de référence --- \")\n",
    "print(\"Base d'apprentissage - MSE : \", mse_train_ref)\n",
    "print(\"Base de test - MSE : \", mse_test_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
